{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa69209-9587-42ab-8cb6-4380fc1f33bc",
   "metadata": {},
   "source": [
    "# **YouTube Data Scrapper Using Selenium**\n",
    "\n",
    "Here i have extracted the data from the particular channel(**@Kurzgesagt**)\n",
    "\n",
    "This Notebook Demonstrates How To Extract:\n",
    "\n",
    "- **Video_link**\n",
    "- **No_of_likes**\n",
    "- **Date_of_upload**\n",
    "- **No_of_views**\n",
    "- **No_of_comments**\n",
    "- **Cleaned_description**\n",
    "- \n",
    "**And Saving Data Into CSV**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c1133-eb8b-4abf-89f6-010c2c98cccb",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17bf65fe-1cf6-41eb-b5fb-abf61797a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impor libraries\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17087d61-a379-4e66-b82b-6306a58a0ebe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Normal request method dont work on websites like youtube and even on most of the websites that's why we have to use a webdriver \n",
    "\n",
    "A WebDriver can do all that because it actually runs a real browser in the background, so the page loads exactly like it does when you open Chrome yourself.\r\n",
    "\r\n",
    "How it works —\r\n",
    "\r\n",
    "1️⃣ You install a WebDriver for your browser, like:\r\n",
    "\r\n",
    "chromedriver for Chrome\r\n",
    "\r\n",
    "geckodriver for Firefox.\r\n",
    "\r\n",
    "2️⃣ You use a tool like Selenium or Playwright to tell that WebDriver what to do.\r\n",
    "\r\n",
    "3️⃣ The WebDriver opens a real browser window for you, does all the clicks & typing, and returns the page’s final HTML — including content loaded by JavaScript!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f02983-7cef-4ad9-a3a4-0079eeac31c8",
   "metadata": {},
   "source": [
    "## Initialize Selenium WebDriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "738f07dd-19a2-4233-9c18-580de205c636",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.youtube.com/@kurzgesagt/videos')\n",
    "\n",
    "#Wait for page to load\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b225d3-4a12-4481-a5c5-c0b2d8f3c892",
   "metadata": {},
   "source": [
    "We usually have to manually scroll down till last video on the video Page Because YouTube uses infinite scrolling. Only a few videos load initially, and more load as you scroll down. If you don’t scroll, Selenium will only find the first few videos.\n",
    "but i have uploaded the code for infinite scroll to reach the bottom of the page , so it  scroll automatically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e9775-a93d-435d-8962-f0bf379b83f3",
   "metadata": {},
   "source": [
    "## Extracting  Video Detailse\n",
    "- Title\n",
    "- Likes\r\n",
    "- Views (exact number)\r\n",
    "- Upload da and timete\r\n",
    "- Desctionrip\n",
    "- Comme_Num\n",
    "- Video_link\n",
    "- Thumbnail_linked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1eafc549-9a77-4790-af39-f793a9604f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223/223 [51:10<00:00, 13.77s/it]\n"
     ]
    }
   ],
   "source": [
    "# Scrolling till bottom of the page automatically\n",
    "last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height\n",
    "\n",
    "#making intial dataframe to collect all videos link and title\n",
    "soup = BeautifulSoup(driver.page_source , 'html.parser')\n",
    "video_block = soup.find_all('ytd-rich-item-renderer')\n",
    "\n",
    "\n",
    "#creating empty list\n",
    "data = []\n",
    "\n",
    "#extracting link and title\n",
    "#--------------------------link and title block------------------------------------\n",
    "\n",
    "for sp in video_block:\n",
    "    try:\n",
    "        title         = sp.find('h3').text\n",
    "    except:\n",
    "        title = np.nan\n",
    "    try:\n",
    "        video_link    = 'https://www.youtube.com'+sp.find('a').get('href')\n",
    "    except:\n",
    "        title = np.nan \n",
    "    try:\n",
    "        thumbnail_link = sp.find('img').get('src').split('?')[0]\n",
    "    except:\n",
    "        thumbnail_link = np.nan \n",
    "        \n",
    "    #appending data and setting default values so nothing breaks if extraction fails     \n",
    "    data.append({\n",
    "        'title':title,\n",
    "        'views':np.nan,\n",
    "        'upload_date':np.nan,\n",
    "        'upload_time':np.nan,\n",
    "        'comments_nubm':np.nan,\n",
    "        'vid_description':np.nan,\n",
    "        'video_link':video_link,\n",
    "        'thumbnail_link':thumbnail_link})\n",
    "\n",
    "\n",
    "#--------------------------link and title block------------------------------------\n",
    "  \n",
    "\n",
    "#Iterating through each video through each video and collecting their data\n",
    "for link in tqdm(data):\n",
    "   \n",
    "    try:\n",
    "        driver.get(link['video_link'])\n",
    "    \n",
    "            \n",
    "        time.sleep(5)\n",
    "    \n",
    "        #extracting Views, Date and time of upload\n",
    "        #--------------------------views, date and time block------------------------------------\n",
    "    \n",
    "        \n",
    "        # Search for the ytInitialPlayerResponse JSON\n",
    "        pattern = r'var ytInitialPlayerResponse = ({.*?});'\n",
    "        match = re.search(pattern,driver.page_source)\n",
    "        \n",
    "        if match:\n",
    "            data_json = json.loads(match.group(1))\n",
    "            # Exact view count\n",
    "            exact_views = data_json['videoDetails']['viewCount']\n",
    "            # Exact upload date (ISO format)\n",
    "            raw_date = data_json['microformat']['playerMicroformatRenderer']['uploadDate']\n",
    "            if raw_date:\n",
    "                dt = datetime.fromisoformat(raw_date)\n",
    "                formatted_date = dt.strftime('%d-%m-%Y')\n",
    "                formatted_time = dt.strftime('%I:%M:%S %p')\n",
    "            \n",
    "                link['views']= exact_views\n",
    "                link['upload_date'] = formatted_date\n",
    "                link['upload_time'] = formatted_time\n",
    "        \n",
    "           \n",
    "       \n",
    "        #--------------------------views, date and time block------------------------------------\n",
    "        \n",
    "        #Extracting Comments\n",
    "        #------------------------------------comments--------------------------------------------\n",
    "        \n",
    "        # Scroll down a bit first (YouTube needs a small scroll to load comments)\n",
    "        try:\n",
    "            driver.execute_script(\"window.scrollBy(0, 10000);\")\n",
    "            \n",
    "            # Wait until the comment count element is present\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # finding the tag where comment num is present , then scrolling to it \n",
    "            comment_section  = driver.find_element(By.CSS_SELECTOR ,'yt-formatted-string.count-text.style-scope.ytd-comments-header-renderer' )\n",
    "            driver.execute_script('arguments[0].scrollIntoView(true);',comment_section)\n",
    "            comment_num = comment_section.text\n",
    "            \n",
    "            #getting exact comment number in int format\n",
    "            link['comments_nubm'] = int(''.join(filter(str.isdigit,comment_num)))\n",
    "            \n",
    "        except:\n",
    "            link['comments_nubm'] = np.nan #comments disabled or not found\n",
    "        #------------------------------------comments--------------------------------------------\n",
    "        \n",
    "        #Extracting Cleaned Description\n",
    "        #---------------------------------Cleaned Description------------------------------------\n",
    "        \n",
    "        # Try to click Show More if it exists\n",
    "        try:\n",
    "            try:\n",
    "                show_more = WebDriverWait(driver, 2).until(\n",
    "                    EC.element_to_be_clickable((By.CSS_SELECTOR, \"tp-yt-paper-button#expand\"))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].click();\", show_more)\n",
    "                #print(\"Clicked Show More button\")\n",
    "            except:\n",
    "                pass #No Show More button\n",
    "                \n",
    "            span = driver.find_elements(By.CSS_SELECTOR , 'span.yt-core-attributed-string--link-inherit-color')\n",
    "            des = ''\n",
    "            \n",
    "            for spans in span:\n",
    "                text = spans.text\n",
    "                if len(spans.text)>300:\n",
    "                    des=  text\n",
    "            \n",
    "            def clear_description(des):\n",
    "                des = ' '.join(des.split())  # normalize whitespace , it does this by spllitting the string whereever whitespaces(\\n , tab , space , etc) emerges , then join all the elements of the string with single space\n",
    "                # Remove junk keywords\n",
    "                junk_keywords = ['OUR CHANNELS', 'Follow us', 'Subscribe', 'http', 'More videos', 'German']\n",
    "                for word in junk_keywords:\n",
    "                    if word in des:\n",
    "                        des = des.split(word)[0]\n",
    "                        break\n",
    "                # Remove relative dates \n",
    "                des = re.sub(r'\\b\\d+\\s+(second|seconds|minute|minutes|hour|hours|day|days|week|weeks|month|months|year|years)\\s+ago\\b', '', des, flags=re.IGNORECASE)\n",
    "                return des.strip()\n",
    "            \n",
    "            link['vid_description']= clear_description(des)\n",
    "        except:\n",
    "            link['vid_description'] = np.nan\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error Processing {link['video_link']}:{e}')\n",
    "        \n",
    "    \n",
    "        #---------------------------------Cleaned Description------------------------------------\n",
    "        \n",
    "        \n",
    "#creating data frame and saving data in it\n",
    "#saving data(list) in a data frame\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('YouTube(kurzgesagt)(2).csv' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1872a3a2-e4e3-4ec8-9ab4-92ee5f272e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>views</th>\n",
       "      <th>upload_date</th>\n",
       "      <th>upload_time</th>\n",
       "      <th>comments_nubm</th>\n",
       "      <th>video_link</th>\n",
       "      <th>thumbnail_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Let's Kill You a Billion Times to Make You Imm...</td>\n",
       "      <td>2464691</td>\n",
       "      <td>29-07-2025</td>\n",
       "      <td>07:00:02 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.youtube.com/watch?v=7wK4peez9zE</td>\n",
       "      <td>https://i.ytimg.com/vi/7wK4peez9zE/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When Your Body Attacks Itself – Autoimmune</td>\n",
       "      <td>2358145</td>\n",
       "      <td>01-07-2025</td>\n",
       "      <td>07:00:01 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.youtube.com/watch?v=efOW5NUTYB8</td>\n",
       "      <td>https://i.ytimg.com/vi/efOW5NUTYB8/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How Nuclear Flies Protect You from Flesh-Eatin...</td>\n",
       "      <td>4741889</td>\n",
       "      <td>03-06-2025</td>\n",
       "      <td>07:00:05 AM</td>\n",
       "      <td>8454.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=zxq60I5RSW8</td>\n",
       "      <td>https://i.ytimg.com/vi/zxq60I5RSW8/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why Does Fentanyl Feel So Good?</td>\n",
       "      <td>6967320</td>\n",
       "      <td>20-05-2025</td>\n",
       "      <td>07:00:01 AM</td>\n",
       "      <td>19392.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=m6KnVTYtSc0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What If It Rains Bananas For A Single Day? (Sp...</td>\n",
       "      <td>4154942</td>\n",
       "      <td>06-05-2025</td>\n",
       "      <td>07:00:01 AM</td>\n",
       "      <td>10662.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=tRXy-b6_lBc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>How The Stock Exchange Works (For Dummies)</td>\n",
       "      <td>8488160</td>\n",
       "      <td>28-11-2013</td>\n",
       "      <td>09:03:32 AM</td>\n",
       "      <td>6902.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=F3QpgXBtDeo</td>\n",
       "      <td>https://i.ytimg.com/vi/F3QpgXBtDeo/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>The Gulf Stream Explained</td>\n",
       "      <td>6204998</td>\n",
       "      <td>11-10-2013</td>\n",
       "      <td>12:11:39 PM</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=UuGrBhK2c7U</td>\n",
       "      <td>https://i.ytimg.com/vi/UuGrBhK2c7U/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Fracking explained: opportunity or danger</td>\n",
       "      <td>7417981</td>\n",
       "      <td>03-09-2013</td>\n",
       "      <td>02:12:24 AM</td>\n",
       "      <td>8109.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=Uti2niW2BRA</td>\n",
       "      <td>https://i.ytimg.com/vi/Uti2niW2BRA/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>The Solar System -- our home in space</td>\n",
       "      <td>6461624</td>\n",
       "      <td>22-08-2013</td>\n",
       "      <td>06:24:56 AM</td>\n",
       "      <td>6116.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=KsF_hdjWJjo</td>\n",
       "      <td>https://i.ytimg.com/vi/KsF_hdjWJjo/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>How Evolution works</td>\n",
       "      <td>11180530</td>\n",
       "      <td>11-07-2013</td>\n",
       "      <td>07:09:52 AM</td>\n",
       "      <td>77137.0</td>\n",
       "      <td>https://www.youtube.com/watch?v=hOfRN0KihOU</td>\n",
       "      <td>https://i.ytimg.com/vi/hOfRN0KihOU/hqdefault.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title     views upload_date  \\\n",
       "0    Let's Kill You a Billion Times to Make You Imm...   2464691  29-07-2025   \n",
       "1           When Your Body Attacks Itself – Autoimmune   2358145  01-07-2025   \n",
       "2    How Nuclear Flies Protect You from Flesh-Eatin...   4741889  03-06-2025   \n",
       "3                      Why Does Fentanyl Feel So Good?   6967320  20-05-2025   \n",
       "4    What If It Rains Bananas For A Single Day? (Sp...   4154942  06-05-2025   \n",
       "..                                                 ...       ...         ...   \n",
       "218         How The Stock Exchange Works (For Dummies)   8488160  28-11-2013   \n",
       "219                          The Gulf Stream Explained   6204998  11-10-2013   \n",
       "220          Fracking explained: opportunity or danger   7417981  03-09-2013   \n",
       "221              The Solar System -- our home in space   6461624  22-08-2013   \n",
       "222                                How Evolution works  11180530  11-07-2013   \n",
       "\n",
       "     upload_time  comments_nubm                                   video_link  \\\n",
       "0    07:00:02 AM            NaN  https://www.youtube.com/watch?v=7wK4peez9zE   \n",
       "1    07:00:01 AM            NaN  https://www.youtube.com/watch?v=efOW5NUTYB8   \n",
       "2    07:00:05 AM         8454.0  https://www.youtube.com/watch?v=zxq60I5RSW8   \n",
       "3    07:00:01 AM        19392.0  https://www.youtube.com/watch?v=m6KnVTYtSc0   \n",
       "4    07:00:01 AM        10662.0  https://www.youtube.com/watch?v=tRXy-b6_lBc   \n",
       "..           ...            ...                                          ...   \n",
       "218  09:03:32 AM         6902.0  https://www.youtube.com/watch?v=F3QpgXBtDeo   \n",
       "219  12:11:39 PM         2010.0  https://www.youtube.com/watch?v=UuGrBhK2c7U   \n",
       "220  02:12:24 AM         8109.0  https://www.youtube.com/watch?v=Uti2niW2BRA   \n",
       "221  06:24:56 AM         6116.0  https://www.youtube.com/watch?v=KsF_hdjWJjo   \n",
       "222  07:09:52 AM        77137.0  https://www.youtube.com/watch?v=hOfRN0KihOU   \n",
       "\n",
       "                                       thumbnail_link  \n",
       "0    https://i.ytimg.com/vi/7wK4peez9zE/hqdefault.jpg  \n",
       "1    https://i.ytimg.com/vi/efOW5NUTYB8/hqdefault.jpg  \n",
       "2    https://i.ytimg.com/vi/zxq60I5RSW8/hqdefault.jpg  \n",
       "3                                                 NaN  \n",
       "4                                                 NaN  \n",
       "..                                                ...  \n",
       "218  https://i.ytimg.com/vi/F3QpgXBtDeo/hqdefault.jpg  \n",
       "219  https://i.ytimg.com/vi/UuGrBhK2c7U/hqdefault.jpg  \n",
       "220  https://i.ytimg.com/vi/Uti2niW2BRA/hqdefault.jpg  \n",
       "221  https://i.ytimg.com/vi/KsF_hdjWJjo/hqdefault.jpg  \n",
       "222  https://i.ytimg.com/vi/hOfRN0KihOU/hqdefault.jpg  \n",
       "\n",
       "[223 rows x 7 columns]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1a38a01b-5835-4d07-a0f2-cf8b56184764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                0\n",
       "views                0\n",
       "upload_date          0\n",
       "upload_time          0\n",
       "comments_nubm        2\n",
       "vid_description      0\n",
       "video_link           0\n",
       "thumbnail_link     129\n",
       "dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c1cfd-f285-4e1a-9231-89069568870b",
   "metadata": {},
   "source": [
    "## **Note:**\n",
    "\n",
    "- I couldn't extract the vid_description properly , most of the description data was NAN as it was a difficult task to click on show more then find the specific description in the whole description sectio.\n",
    "- **Though** i still have that part in the code , if in case anyone wants to know i attempted that.\n",
    "- With slight modification this code can be applied to any YouTube channel to scrap most of the details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a52fb0-4f75-4c21-b661-1c6b31cae119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
